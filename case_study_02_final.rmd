---
title: "CaseStudy02-Final"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction
For this case study, we investigated the CaseStudy02 dataset. The dataset contained 870 entries with 36 feature vectors. Through the analysis, we were able to classify potential `Attrition` for employees with greater than 70% accuracy as well as predict income rates with an RMSE of 1387. In addition, we overcame several unforeseen challenges in the dataset. 

## Exploratory Data Analysis 
In this section we will perform an exploratory data analysis (EDA) of the CaseStudy-2 dataset. This dataset can be found in the `datasets` directory for the project. The project will attempt to investigate the data by performing the following steps:

 - Determine the dimensions of the dataset.

 - Address non-informative / missing features. 

 - Compute the correlation values between numeric features and the `Attrition` variable.

 - Comment on the distribution of `Attrition` variable.
 
 - Compute the correlation values between categorical features and the `Attrition` variable.

 - Identify any relationship between the `Attrition` variable and other variables in the dataset.

 - Compute the correlation values between numeric features and the `MonthlyIncome` variable. 

 - Identify any relationship between the `MonthlyIncome` variable and other variables in the dataset.

### 1. Determine the dimensions of the dataset
The dataset contains 870 entries with 36 features.

```{r message=FALSE, warning=FALSE}
#get the data from the file
caseStudy2DF <- read.csv('datasets\\CaseStudy2-data.csv')

#get the dimensions
dim(caseStudy2DF)

#view a few
head(caseStudy2DF)

#get the classes for each feature vector
sapply(caseStudy2DF, class)
```


### 2. Address non-informative / missing features
The dataset does not contain missing values that need to be compensated for. However, several features have been identified as non-informative. They need to be removed for simplicity. 

```{r message=FALSE, warning=FALSE}
#required libraries
library(DataExplorer)
library(dplyr)

#look for missing values
plot_missing(caseStudy2DF, title='Misssing data points')

#ok no missing values, lets look for all the same values
badCols = caseStudy2DF %>% summarise_all(funs(n_distinct(.))) %>% select_if(. == 1)
names(badCols)

#we have 3 cols that just have one value in it 
#create a new df without them
caseStudyMin = caseStudy2DF[, -which(names(caseStudy2DF) %in% names(badCols))]
dim(caseStudyMin)
```

### 3. Compute the correlation values between numeric features and the Attrition variable

```{r message=FALSE, warning=FALSE}
#required libraries
library(purrr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(corrly)
library("PerformanceAnalytics")

#get an overview of each feature vector
caseStudyMin %>% keep(is.numeric) %>% gather() %>% ggplot(aes(value)) + facet_wrap(~ key, scales = "free") + geom_histogram()

#split the dataset into yes and no for attrition 
attNoDf <- filter(caseStudyMin, caseStudyMin$Attrition == 'No') %>% keep(is.numeric)
attYesDf <- filter(caseStudyMin, caseStudyMin$Attrition == 'Yes') %>% keep(is.numeric)

#get the size of each sample population 
dim(attNoDf)
dim(attYesDf)

#view the correlations for the no population
matrixly(data=attNoDf)

#view the correlations for the yes population
matrixly(data=attYesDf)

#convert the yes and no into ints and view the entire population
numericAttritionDf <- caseStudyMin %>% mutate(Attrition = ifelse(as.character(Attrition) == "Yes", 1, as.character(Attrition)))
numericAttritionDf <- numericAttritionDf %>% mutate(Attrition = ifelse(as.character(Attrition) == "No", 0, as.numeric(Attrition)))
numericAttritionDf <- numericAttritionDf %>% keep(is.numeric)
matrixly(data=numericAttritionDf)

#get the rounded correlation data as a table
res <- cor(numericAttritionDf)
round(res, 2)

#plot the correlation data with respect to attrition for each feature vector 
att <- select(numericAttritionDf, c('Attrition','ID','Age','DailyRate', 'DistanceFromHome', 'Education'))
chart.Correlation(att, histogram=TRUE, pch=19)

#plot the correlation data with respect to attrition for each feature vector
att <- select(numericAttritionDf, c('Attrition','EmployeeNumber', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel'))
chart.Correlation(att, histogram=TRUE, pch=19)

#plot the correlation data with respect to attrition for each feature vector
att <- select(numericAttritionDf, c('Attrition','JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike'))
chart.Correlation(att, histogram=TRUE, pch=19)

#plot the correlation data with respect to attrition for each feature vector
att <- select(numericAttritionDf, c('Attrition','PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear'))
chart.Correlation(att, histogram=TRUE, pch=19)

#plot the correlation data with respect to attrition for each feature vector
att <- select(numericAttritionDf, c('Attrition','WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'))
chart.Correlation(att, histogram=TRUE, pch=19)
```

### 4. Comment on the distribution of Attrition variable
The distribution of the `Attrition` variable indicates that we have an imbalanced dataset. Only 16% of the population contains `Attrition=Yes`. 

```{r message=FALSE, warning=FALSE}

#quick look at the distribution of the Attrition variable
hist(numericAttritionDf$Attrition, xlim=c(0,1), xlab='No vs Yes', main=paste('Histogram of Attrition')) 

# % of observations with attrition == YES
dim(filter(numericAttritionDf, numericAttritionDf$Attrition == 1))[1] / (dim(filter(numericAttritionDf, numericAttritionDf$Attrition == 0))[1] + dim(filter(numericAttritionDf, numericAttritionDf$Attrition == 1))[1])

```

### 5. Compute the correlation values between categorical features and the Attrition variable 

```{r message=FALSE, warning=FALSE}
#convert the categorical variables to numbers so we can plot them
factAttritionDF <- caseStudyMin %>% keep(is.character) 
factAttritionDF <- factAttritionDF %>% mutate_if(is.character, as.factor)
factAttritionDF <- factAttritionDF %>% mutate_if(is.factor, as.numeric)

#not needed 
#summary(factAttritionDF)

#plot the correlation data with respect to attrition for each feature vector
chart.Correlation(factAttritionDF, histogram=TRUE, pch=19)
```

### 6. Identify any relationship between the Attrition variable and other features in the dataset
Using the correlation data listed above we will create a new data frame to contain features that have showed some relationship to the `Attrition` variable.  

```{r message=FALSE, warning=FALSE}
#features we found from the eda
caseStudyFeatures <- select(caseStudyMin, c('ID','Attrition','Age', 'DistanceFromHome', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'NumCompaniesWorked', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear','WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsWithCurrManager', 'Department', 'JobRole', 'MaritalStatus', 'OverTime', 'Gender', 'Education', 'YearsSinceLastPromotion'))

#show the selected items
names(caseStudyFeatures)
```


### 7. Compute the correlation values between numeric features and the MonthlyIncome variable

```{r message=FALSE, warning=FALSE}
numCaseStudyDf <- caseStudyMin %>% keep(is.numeric)

#plot the correlation data with respect to MonthlyIncome for each feature vector
att <- select(numCaseStudyDf, c('MonthlyIncome','ID','Age','DailyRate', 'DistanceFromHome', 'Education'))
chart.Correlation(att, histogram=TRUE, pch=19)

#plot the correlation data with respect to MonthlyIncome for each feature vector
att <- select(numCaseStudyDf, c('MonthlyIncome','EmployeeNumber', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel'))
chart.Correlation(att, histogram=TRUE, pch=19)

#plot the correlation data with respect to MonthlyIncome for each feature vector
att <- select(numCaseStudyDf, c('MonthlyIncome','JobSatisfaction', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike'))
chart.Correlation(att, histogram=TRUE, pch=19)

#plot the correlation data with respect to MonthlyIncome for each feature vector
att <- select(numCaseStudyDf, c('MonthlyIncome','PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear'))
chart.Correlation(att, histogram=TRUE, pch=19)

#plot the correlation data with respect to MonthlyIncome for each feature vector
att <- select(numCaseStudyDf, c('MonthlyIncome','WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'))
chart.Correlation(att, histogram=TRUE, pch=19)

```


### 8. Identify any relationship between the MonthlyIncome variable and other variables in the dataset
The `JobLevel` and `TotalWorkingYears` variables appear to be highly correlated with the `MonthlyIncome` variable. 

## Modeling

 - Build a classification model to predict `Attrition` with >60% specificity and >60% selectivity. 
 
 - Build a regression model to predict income with an RMSE < $3000.
 
### 1. Build a classification model to predict attrition with >60% specificity and >60% selectivity
For classification we will train a Naive Bayes classifier. The features used for training the model were identified by their relationship to the `Attrition` variable. Additionally, we use down sampling and invert the percentage of training samples versus their respective population. As a result, we use ~70% of the `Attrition=yes` samples and ~30% of the `Attrition=No` samples to build a training set containing ~30% of the data available. The remaining ~70% of the dataset is used for testing. 

This process is executed 100 times with different seed values. 

```{r message=FALSE, warning=FALSE}
#required libraries
library('e1071')
library(caret)

#features we found from the eda
caseStudyFeatures <- select(caseStudyMin, c('ID','Attrition','Age', 'DistanceFromHome', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'NumCompaniesWorked', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear','WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsWithCurrManager', 'Department', 'JobRole', 'MaritalStatus', 'OverTime', 'Gender', 'Education', 'YearsSinceLastPromotion'))

#we will invert the % of the training population vs actual population for training the model 
200 / sum(caseStudyFeatures$Attrition == 'No') 
100 / sum(caseStudyFeatures$Attrition == 'Yes')

#run the model 100 times to get an average of what it can do
iterations = 100
accs = data.frame(accuracy = numeric(iterations), specificity = numeric(iterations), sensitivity = numeric(iterations))

#do 100 random runs of the model 
for(i in 1:iterations)
{
  #use the counter as a seed so we start from different places each time
  set.seed(i)
  
  #get 200 random no samples
  tmp_n <- filter(caseStudyFeatures, caseStudyFeatures$Attrition == 'No') %>% sample_n(., 200)
  
  #get 100 random yes samples
  tmp_y <- filter(caseStudyFeatures, caseStudyFeatures$Attrition == 'Yes') %>% sample_n(., 100)
  
  #combine the yes and no samples
  training <- merge(tmp_n, tmp_y, all=TRUE)

  #remove our testing samples from the training pop
  testing <- caseStudyFeatures %>% filter(!ID %in% training$ID)
  testing <- testing[,!names(testing) %in% c('ID')]

  training <- training[,!names(training) %in% c('ID')]

  #train the model
  model <- naiveBayes(Attrition~.,data = training)
  p <- predict(model, testing, type='raw')
  
  #save the results 
  cm <- confusionMatrix(table(predict(model,testing),testing$Attrition))
  accs$accuracy[i] <- cm$overall[1]
  accs$sensitivity[i] <- cm[4]$byClass[1]
  accs$specificity[i] <- cm[4]$byClass[2]
  accs$index[i] <- i
}

#get the avg accuracy
mean(accs$accuracy)

#get the avg specificity
mean(accs$specificity)

#get the avg sensitivity
mean(accs$sensitivity)

#inspect the mean accuracy of the model 
accs %>% ggplot(aes(x=index, y=accuracy)) + geom_point() +geom_smooth(method = lm) + ggtitle('Bayes Classification (accuracy)')

#inspect the mean specificity of the model 
accs %>% ggplot(aes(x=index, y=specificity)) + geom_point() +geom_smooth(method = lm) + ggtitle('Bayes Classification (specificity)')

#inspect the mean sensitivity of the model 
accs %>% ggplot(aes(x=index, y=sensitivity)) + geom_point() +geom_smooth(method = lm) + ggtitle('Bayes Classification (sensitivity)')

#just take the last model 
#and use it
p <- predict(model, caseStudyFeatures, type='raw')

#append the predictions
t <- data.frame(caseStudyMin, p)
colnames(t)[35] <- "Prediction"

#set the string values
o  <- t %>% mutate(Prediction = ifelse(Prediction >= .5, 'Yes', 'No'))
outdata <- select(o, c('ID', 'Prediction'))

#rename
colnames(outdata)[2] <- 'Attrition'

#dont do this on the website
#c <- outdata[with(outdata, order(ID)),]
#write.csv(c,"..\\predictions\\CaseStudy2PredictionsClassify.csv", row.names = FALSE)
```

### 2. Build a regression model to predict income with an RMSE < $3000

```{r message=FALSE, warning=FALSE}
#required libraries
library(Metrics)

#plot the data to see what we are working with
numCaseStudyDf %>% ggplot(aes(x=TotalWorkingYears, y=MonthlyIncome)) + geom_point() +ggtitle('TotalWorkingYears vs MonthlyIncome') + geom_smooth(method = lm)

#create a new df for our highly correlated variables
incomeDF <- data.frame(numCaseStudyDf$TotalWorkingYears, numCaseStudyDf$MonthlyIncome, numCaseStudyDf$JobLevel)
names(incomeDF)[1] <- 'TotalWorkingYears'
names(incomeDF)[2] <- 'MonthlyIncome'
names(incomeDF)[3] <- 'JobLevel'

#train the regression model 
lmh <- lm(incomeDF$MonthlyIncome ~ incomeDF$TotalWorkingYears+incomeDF$JobLevel, data = incomeDF)

#good p-values for TotalWorkingYears and JobLevel
summary(lmh)

#get the data for our rmse calculation 
dataB <- incomeDF[, c("MonthlyIncome", "TotalWorkingYears", "JobLevel")]

#our residuals dont show a pattern indicating we have a good model 
plot(lmh$residuals, pch=16, col='blue', xlab='index', ylab='residual value')

#build a prediction to test the model 
predDf <- data.frame(MonthlyIncome <- c(8333), JobLevel <- c(1))

#predict and get the rmse
rmse(dataB$MonthlyIncome, predict(lmh, newdata=predDf))
```

